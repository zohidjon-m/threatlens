import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import joblib
import os
import sys
import hashlib 
from pymongo import MongoClient
import time
import shap
import math

# --- CONFIGURATION AND PATHS ---
# NOTE: Adjust this path to where you placed the EMBER data files (e.g., X_train.npy, y_train.npy)
EMBER_DATA_PATH = "C:/Users/Natalia Perez/threatlens/Natalia_ML_Shap/ember_data/" 

FEATURES_FILE = os.path.join(EMBER_DATA_PATH, 'train_ember_2018_v2_features.parquet')
#LABELS_FILE = os.path.join(EMBER_DATA_PATH, 'train_labels.csv')
MODEL_OUTPUT_PATH = "lgbm_malware_detector.pkl"
MODEL_PATH = MODEL_OUTPUT_PATH
RANDOM_SEED = 42
TOTAL_EMBER_FEATURES = 2381

# Configuration for MongoDB
MONGO_URI = "mongodb+srv://bigdata-project0:5yQbyhWiQeAWtDQ2@cluster0.pydvesz.mongodb.net/?appName=Cluster0" 
DB_NAME = "bigdata-project0"
COLLECTION_NAME_SCORES = "ml_scores" 
COLLECTION_NAME_TI = "threat_intel"


def calculate_static_features(file_path):
    """
    Calculates static features (size, entropy, histogram) and 
    vectorizes them into a 2381-feature array for LightGBM.
    
    Returns: (feature_vector_2D, sha256)
    """
    
    # 256 counters initialized to zero for the byte histogram (0x00 to 0xFF)
    byte_histogram = np.zeros(256, dtype=int)
    file_size = 0
    entropy = 0.0
    
    try:
        # Read the entire file
        with open(file_path, 'rb') as f:
            data = f.read()
        
        file_size = len(data)
        
        # 1. Byte Histogram & Entropy Setup
        for byte in data:
            byte_histogram[byte] += 1
        
        # 2. Calculate Shannon Entropy
        # H = - sum(p * log2(p))
        for count in byte_histogram:
            if count > 0:
                probability = count / file_size
                entropy -= probability * math.log2(probability)
        
    except Exception as e:
        print(f"Error reading file {file_path}: {e}")
        return None, None
    
    # 3. EMBER Vectorization (Crucial for the model)
    
    # Normalize the histogram to frequencies (probabilities)
    hist_norm = byte_histogram / file_size
    
    # Create the base 2381-feature vector (filled with zeros)
    # TOTAL_EMBER_FEATURES must be defined as 2381 globally
    feature_vector = np.zeros(TOTAL_EMBER_FEATURES, dtype=np.float32)
    
    # Place the basic features in the first positions expected by the model:
    # Index 0: File size
    # Index 1: Entropy
    # Index 2-257: 256-byte Histogram
    feature_vector[0] = file_size
    feature_vector[1] = entropy
    feature_vector[2:258] = hist_norm
    
    # 4. Calculate SHA256 (for MongoDB key)
    sha256 = hashlib.sha256(data).hexdigest()
    
    # The final prediction vector must be reshaped to (1, 2381)
    return feature_vector.reshape(1, -1), sha256

# --- LOAD FUNCTION UPDATED ---
def load_ember_data():
    print("Loading data using Pandas (COMPLETE Parquet format)...")
    
    try:
        # 1. Load the COMPLETE Parquet file
        df_complete = pd.read_parquet(FEATURES_FILE) 
        print(f"Complete DataFrame loaded. Shape: {df_complete.shape}")
        
        # 2. Separate Labels (y) and Features (X)
        y_all = df_complete['label'].values
        X_df = df_complete.drop(columns=['label']) 
        
        # 3. Clean and Align Data
        # Filter out the unlabeled samples (labels == -1)
        valid_indices = y_all != -1
        
        # Filter both X and y
        X = X_df[valid_indices].values
        y = y_all[valid_indices]
        
    except FileNotFoundError:
        print(f"--- CRITICAL ERROR: FILE NOT FOUNDED ---")
        print(f"Searched route: {FEATURES_FILE}")
        print("Make sure Parquet file exists in './ember_data/'. folder")
        return None, None
    except Exception as e:
        print(f"An error occurred during data loading: {e}")
        return None, None
    
    print(f"Data successfully processed. Total valid samples (0/1): {len(X)}")
    print(f"Features (X) shape after cleaning: {X.shape}")
    return X, y

# --- MODEL TRAINING FUNCTION ---

def train_lgbm_model(X, y):
    """
    Trains the LightGBM classifier using the loaded EMBER features.
    """
    print("\nStarting LightGBM model training...")
    
    # 1. Data Splitting: 80% Train, 20% Test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y
    )
    
    # 2. LightGBM Configuration (optimized for binary classification)
    lgbm_params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'n_estimators': 1000, 
        'learning_rate': 0.05,
        'seed': RANDOM_SEED,
        'n_jobs': -1, # Use all available cores
        'verbose': -1 # Suppress detailed output during training
    }
    
    # 3. Model Training with Early Stopping
    model = lgb.LGBMClassifier(**lgbm_params)
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        eval_metric='auc',
        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)]
    )
    
    # 4. Final Evaluation (Focusing on AUC, a robust metric for imbalanced data)
    test_proba = model.predict_proba(X_test)[:, 1] # Probability of being malware (class 1)
    auc_score = roc_auc_score(y_test, test_proba)
    
    print(f"\n--- Training Complete ---")
    print(f"Area Under Curve (AUC) Score on Test Set: {auc_score:.4f}")
    
    return model

def calculate_shap_explanations(model, feature_vector):
    """Calculates SHAP values (contributions) to explain the prediction."""
    
    explainer = shap.TreeExplainer(model)
    
    # SHAP CORRECTION: Get the output from shap.shap_values
    shap_output = explainer.shap_values(feature_vector)
    
    # Handle the binary output of LightGBM/SHAP:
    if isinstance(shap_output, list) and len(shap_output) > 1:
        # Case 1: Classic output [Class 0, Class 1]. Take Class 1 array (malware)
        shap_values = shap_output[1]
    else:
        # Case 2: Simplified output (common for binary), use the array directly
        shap_values = shap_output 
        
    # The SHAP array is 2D (1, 2381). We access the first element (index 0)
    # to get the 2381-value vector.
    
    # Get the top 5 largest absolute values (most important features)
    abs_shap_values = np.abs(shap_values[0])
    top_5_indices = np.argsort(abs_shap_values)[-5:]
    
    # Create the list of dictionaries to save in MongoDB
    shap_explanations = []
    for i in top_5_indices:
        shap_explanations.append({
            "feature_index": int(i),
            "shap_value": float(shap_values[0][i]),
            # NOTE: In a real scenario, you would include the actual feature name here
        })
        
    return shap_explanations

def run_malware_inference_pipeline(file_path):
    """Runs the entire pipeline: extraction, inference, and SHAP."""
    
    print(f"\n--- Starting Analysis of {os.path.basename(file_path)} ---")
    
    # 1. Extract features and SHA256
    feature_vector, file_hash = calculate_static_features(file_path)
    if feature_vector is None:
        return None
        
    # 2. Load the model
    try:
        model = joblib.load(MODEL_PATH)
    except FileNotFoundError:
        print(f"Error: Model not found at {MODEL_PATH}. Check path.")
        return None
        
    # 3. Predict the probability (malware_prob)
    raw_prob = model.predict_proba(feature_vector)[:, 1][0]

    # --- IMPLEMENTATION OF THRESHOLD ---
    CUSTOM_THRESHOLD = 0.25 

    if raw_prob >= CUSTOM_THRESHOLD:
        # If probability is higher than 0.25, it reports as HISH RISK (almost 1.0)
        # This is what will be save in DB.
        malware_prob = 0.95 # fixed value of HIGH RISK
        print(f"DEBUG: Score adjusted (Threshold {CUSTOM_THRESHOLD}) from {raw_prob:.4f} to {malware_prob:.4f}")
    else:
        # If probability is less than 0.25, it reports the raw value (or low risk)
        malware_prob = raw_prob

    # 4. Calculate SHAP explanations (Week 3)
    shap_data = calculate_shap_explanations(model, feature_vector)

    print(f"FINAL THREAT SCORE (malware_prob): {malware_prob:.4f}")
    print(f"File SHA256: {file_hash}")
    print('HOLA')
    # Return all data for MongoDB
    return {
        "sha256": file_hash,
        "malware_prob": malware_prob,
        "shap_explanations": shap_data,
        "timestamp": time.time()
    }
    

def write_to_mongodb(analysis_data):
    """Saves the ML score and SHAP data to the 'ml_scores' MongoDB collection."""
    
    # Use the MongoDB connection
    try:
        client = MongoClient(MONGO_URI)
        db = client[DB_NAME]
        collection = db[COLLECTION_NAME_SCORES]
        
        # Create the document to save
        score_document = {
            "sha256": analysis_data["sha256"],
            "malware_prob": float(analysis_data["malware_prob"]),
            "shap_explanations": analysis_data["shap_explanations"],
            "timestamp": analysis_data["timestamp"]
        }
        
        # Insert or update
        collection.update_one(
            {"sha256": analysis_data["sha256"]},
            {"$set": score_document},
            upsert=True
        )
        print(f"SUCCESS: Analysis results saved for hash {analysis_data['sha256'][:8]}...")
        
    except Exception as e:
        print(f"ERROR: Could not write to MongoDB. Check connection URI. Error: {e}")
    finally:
        if 'client' in locals():
            client.close()

# --- MAIN EXECUTION ---

if __name__ == '__main__':
   # Define the path for the test file (REAL extracted file)
    TEST_FILE_PATH = "zeek_data/extracted_files/extract-1518803127.86051-HTTP-FbWryLVFTfvjPtaVh" 
    
    # --- Safety Logic: Train model if it doesn't exist ---
    #if not os.path.exists(MODEL_PATH):
    #    print("WARNING: Model not found. Training model first...")
    #    X, y = load_ember_data()
    #    trained_model = train_lgbm_model(X, y)
    #    joblib.dump(trained_model, MODEL_PATH)
    #    print("Model training completed and saved.")
    
    # --- Week 2/3 Task: Run Inference ---
    
    # If a real file is not available, this will create a temporary dummy file.
    if not os.path.exists(TEST_FILE_PATH):
        print("\nWARNING: Test file not found at the specified path. Creating a DUMMY file.")
        TEST_FILE_PATH = "dummy_test_file.bin"
        with open(TEST_FILE_PATH, 'wb') as f:
            f.write(os.urandom(1024 * 50)) # 50KB of random data
    
    # Run the entire analysis pipeline
    analysis_results = run_malware_inference_pipeline(TEST_FILE_PATH)
    
    if analysis_results:
        # 5. Save results to MongoDB
        write_to_mongodb(analysis_results)
        print("\nPipeline complete. Results available in MongoDB.")