from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
from motor.motor_asyncio import AsyncIOMotorClient
import os
from dotenv import load_dotenv
import pandas as pd
import io
from datetime import datetime, timezone

load_dotenv()

app = FastAPI(title="ThreatLens Malware Detection System API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- DATABASE CONNECTION ---
# Connect to the shared team database 'logsdb'
MONGODB_URI = os.getenv("MONGODB_URI", "mongodb://localhost:27017")
client = AsyncIOMotorClient(MONGODB_URI)
db = client.logsdb  # <--- CHANGED FROM 'threatlens' TO 'logsdb'

# Map to existing collections from Members 1, 2, 3
network_collection = db.network_anomalies  # Created by Member 2
files_collection = db.files                # Created by Member 1/3
alerts_collection = db.alerts              # Created by YOU (Member 4)
datasets_collection = db.datasets          # Your raw uploads (optional now)

# --- ENDPOINTS ---

@app.get("/")
async def read_root():
    try:
        # Check connection to logsdb
        count = await network_collection.count_documents({})
        return {"status": "Connected to logsdb", "network_records": count}
    except Exception as e:
        return {"status": "Error", "details": str(e)}

@app.get("/alerts")
async def get_alerts(limit: int = 100):
    """Fetch alerts generated by your correlation engine"""
    alerts = await alerts_collection.find().sort("timestamp", -1).limit(limit).to_list(limit)
    for alert in alerts:
        alert['_id'] = str(alert['_id'])
    return alerts

@app.get("/file/{sha256}")
async def get_file_details(sha256: str):
    """Fetch file analysis from Member 3's data"""
    file_doc = await files_collection.find_one({"sha256": sha256})
    if not file_doc:
        # If not found in DB, return a "Not Found" structure so dashboard doesn't crash
        # This allows the dashboard to show "File safe" or "Unknown"
        raise HTTPException(status_code=404, detail="File analysis not found")
    
    file_doc['_id'] = str(file_doc['_id'])
    return file_doc

@app.get("/network/{ip}")
async def get_network_stats(ip: str):
    """Fetch anomaly scores from Member 2's data"""
    # Member 2's collection uses "ip" as the key
    network_doc = await network_collection.find_one({"ip": ip})
    if not network_doc:
        raise HTTPException(status_code=404, detail="Host data not found")
    
    network_doc['_id'] = str(network_doc['_id'])
    return network_doc

@app.get("/network-all")
async def get_all_network_data(limit: int = 300):
    """
    Fetch network nodes for topology view + Total Count.
    """
    try:
        # 1. Get Total Count of all records in the collection
        total_count = await network_collection.count_documents({})
        
        # 2. Fetch the top 'limit' most anomalous IPs
        cursor = network_collection.find().sort("anomaly_score", -1).limit(limit)
        nodes = await cursor.to_list(length=limit)
        
        for n in nodes:
            n['_id'] = str(n['_id'])
            if 'features' not in n: n['features'] = {}
                
        return {
            "total_count": total_count,
            "nodes": nodes
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch topology: {str(e)}")
    
@app.post("/run-analysis")
async def run_analysis():
    """
    CORRELATION ENGINE:
    Reads Member 2's anomalies and Member 3's file scores from 'logsdb',
    combines them, and creates Alerts.
    """
    try:
        # 1. Get high-risk IPs from Member 2 (Network Anomalies)
        # We look for any IP with anomaly_score > 0.5
        cursor = network_collection.find({"anomaly_score": {"$gt": 0.5}}).limit(50)
        suspicious_ips = await cursor.to_list(length=50)
        
        new_alerts = []
        
        for net_record in suspicious_ips:
            ip = net_record.get('ip')
            score = net_record.get('anomaly_score', 0)
            
            # 2. Logic: If anomaly score is high, generate an alert
            if score > 0.7:
                severity = "critical"
            else:
                severity = "medium"
                
            alert_doc = {
                "ip": ip,
                "sha256": "N/A (Network Anomaly)",
                "threat_score": score,  # Use the anomaly score as threat score
                "severity": severity,
                "type": "Network Anomaly",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "details": f"High anomaly score ({score:.2f}) detected by Spark model."
            }
            new_alerts.append(alert_doc)
            
        # 3. Save generated alerts to your collection
        if new_alerts:
            # Clear old alerts first to avoid duplicates for the demo
            await alerts_collection.delete_many({})
            await alerts_collection.insert_many(new_alerts)
            
        return {"message": f"Correlated {len(new_alerts)} alerts from logsdb."}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)